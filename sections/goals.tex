\section{Research Goals}
Given the conflicting prior results on how interaction impacts Bayesian reasoning visualizations, the overarching goal of this paper is to empirically test whether adding interaction to a static visualization (i.e. a visualization with no occluded or hidden data that does not require interaction for data exploration) can improve performance on a Bayesian reasoning task. We hypothesize that the mixed results of prior work are partially due to confounding factors between experiments, such as underlying visualization designs and users' spatial ability. %Additionally, prior research has shown that spatial ability plays a strong role in a person's ability to use a visualization, and more specifically, in accuracy on Bayesian reasoning tasks~\cite{liu2020Survey, ottley2016Bayesian}. Thus, we postulate that the effectiveness of interaction also depends on a user's spatial ability. 

The following research questions guide our investigation:
\begin{compacthang}
	\item \textbf{RQ1}: Does adding interaction to a static visualization improve accuracy on a Bayesian reasoning task? 
	\item \textbf{RQ2}: Is the effect of interaction modulated by the effectiveness of the underlying static visualization? 
	\item \textbf{RQ3}: Does users' spatial ability moderate performance on a Bayesian reasoning task with an interactive visualization?   
\end{compacthang}

%We present the results to a crowdsourced study. For the study we design three static (or base) visualizations based on varying theories for visualizing a Bayesian reasoning problem. Using work by Tsai et al.~\cite{tsai2011Interactive} (which found users given interactive Bayesian reasoning visualizations outperformed those given probabilistic Bayesian reasoning text on a Bayesian reasoning task) as a starting point, we add interaction to each base visualization through checkboxes. The checkboxes allow users to hide and un-hide key pieces of the visualization. We compare participants' accuracy with the interactive checkbox visualizations to accuracy with static visualizations, and expect participants assigned the interactive visualization will perform the Bayesian reasoning task with higher accuracy than participants assigned the static visualization. However, we expect base visualization design and users' spatial ability to moderate performance gains. This experiment is a step towards a deeper understanding of interaction in visualization. By empirically demonstrating what specific factors lead to performance gains and losses when making a static visualization interactive, we hope to lay the ground work for better interactive visualization design with evidence-based design guidelines. 

We present the results to a crowdsourced study. For the study we design three static (or base) visualizations based on varying theories for visualizing a Bayesian reasoning problem. We add interaction to each base visualization through checkboxes and compare participants' accuracy with the interactive checkbox visualizations to accuracy with static visualizations. We expect participants assigned the interactive visualization will perform the Bayesian reasoning task with higher accuracy than participants assigned the static visualization. However, we expect base visualization design and users' spatial ability to moderate performance gains.  
This experiment is a step towards a deeper understanding of interaction in visualization. By empirically demonstrating what specific factors lead to performance gains and losses when making a static visualization interactive, we hope to lay the ground work for better interactive visualization design with evidence-based design guidelines. 

